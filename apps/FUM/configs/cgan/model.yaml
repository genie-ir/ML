generator:
  metrics: 
    - gloss
  optimizer:
    blf: 50
    params:
      lr: 0.002
  params:
    forward:
      - seq0
      - seq1
      - concat:
          input:
            - gen_part1
            - gen_part2
          output: x
          params:
            dim: 1
      - seq2
      - Return: x
    seq0:
      input: y
      output: gen_part2
      head:
        - .long
        - ..F.one_hot:
            num_classes: $nclass
        - .float
        - .view:
            - -1
            - $nclass
            - 1
            - 1
      tail:
        - torch.nn.ConvTranspose2d:
            in_channels: $nclass
            out_channels: $seq2.f[0]//2
            kernel_size: 4
            stride: 1
        - torch.nn.BatchNorm2d:
            num_features: $seq2.f[0]//2
        - torch.nn.ReLU:
            inplace: True
    seq1:
      input: z
      output: gen_part1
      head:
        - torch.nn.ConvTranspose2d:
            in_channels: $zdim  
            out_channels: $seq2.f[0]//2
            kernel_size: 4
            stride: 1
        - torch.nn.BatchNorm2d:
            num_features: $seq2.f[0]//2
        - torch.nn.ReLU:
            inplace: True
    seq2:
      input: x
      output: x
      tail:
        - BB.CCR:
            archcode: 1
            C0:
              in_channels: $ncluster
              out_channels: $ncluster
              kernel_size: 3
              stride: 1
              padding: 1
              bias: False
            C1:
              in_channels: $ncluster
              out_channels: $ncluster
              kernel_size: 1
              stride: 1
              padding: 0
              bias: False      
        - torch.nn.Sigmoid
        - torch.nn.Softmax:
            dim: 1
      supernode:
        - BB.TCCRC:
            T:
              in_channels: $seq2.f[i]
              out_channels: $seq2.f[i]
              kernel_size: 4
              stride: 2
              padding: 1
              bias: False
            C:
              kernel_size: 3
              stride: 1
              padding: 1
              bias: False
      f:
        - 64
        - 256
        - 1024

discriminator:
  target: apps.FUM.models.model.D
  metrics: 
    - dloss
  optimizer:
    elf: 50
    params:
      lr: 0.002
  params:
    dimout: 16
    Xdim: 256
    forward:
      - seq0
      - seq1
      - concat:
          input:
            - input_d_part1
            - input_d_part2
          output: x
          params:
            dim: 1
      - seq2
      - Return: realness
    seq0:
      input: y
      output: input_d_part1
      head:
        - .long
        - ..F.one_hot:
            num_classes: $nclass
        - .float
        - .view:
            - -1
            - $nclass
            - 1
            - 1
        - .expand:
            - -1
            - -1
            - $nch
            - $nch
      tail:
        - torch.nn.Conv2d:
            in_channels: $nclass
            out_channels: $dimout
            kernel_size: 4
            stride: 2
            padding: 1
        - torch.nn.BatchNorm2d:
            num_features: $dimout
        - torch.nn.ReLU
    seq1:
      input: x
      output: input_d_part2
      head:
        - torch.nn.Conv2d:
            in_channels: $ncluster  
            out_channels: $dimout
            kernel_size: 4
            stride: 2
            padding: 1
        - torch.nn.BatchNorm2d:
            num_features: $dimout
        - torch.nn.ReLU
    seq2:
      input: x
      output: realness
      tail:
        - BB.CCR:
            archcode: 0
            C0:
              in_channels: $seq2.f[-2]
              out_channels: $seq2.f[-1]
              kernel_size: 4
              stride: 2
              padding: 1
              bias: False
            C1:
              in_channels: $seq2.f[-1]
              out_channels: $seq2.f[-1]
              kernel_size: 1
              stride: 1
              padding: 0
              bias: False
        - torch.nn.Tanh
      pop_supernode: 1
      supernode:
        - BB.CCR:
            archcode: 0
            C0:
              in_channels: $seq2.f[i]
              out_channels: $seq2.f[i+1]
              kernel_size: 4
              stride: 2
              padding: 1
              bias: False
            C1:
              in_channels: $seq2.f[i+1]
              out_channels: $seq2.f[i+1]
              kernel_size: 3
              stride: 1
              padding: 1
              bias: False
      f:
        - 32
        - 64
        - 128
        - 256
